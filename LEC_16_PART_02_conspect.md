# Логистическая регрессия в машинном обучении

## Постановка задачи

Рассмотрим задачу бинарной классификации с использованием модели логистической регрессии. Дана обучающая выборка $(X, Y)$, где $X$ — объекты-признаки, а $Y$ — целевая переменная.

## Адаптация линейной регрессии

Для решения задачи бинарной классификации можно адаптировать метод линейной регрессии. Запишем линейную модель в виде скалярного произведения:

![](images/LEC_16_PART_02/000050s_top_1.jpg)

Уравнение задаёт гиперплоскость в некотором пространстве. Гиперплоскость — это аналог прямой в пространстве размерности больше чем 2.

![](images/LEC_16_PART_02/000070s_top_3.jpg)

Если скалярное произведение меньше 0, то объект находится слева от гиперплоскости (синие объекты). Если скалярное произведение больше 0, то объект находится справа от гиперплоскости (красные объекты).

```mermaid
graph LR
    A[Скалярное произведение < 0] --> B[Объект слева от гиперплоскости]
    C[Скалярное произведение > 0] --> D[Объект справа от гиперплоскости]
```

## Понятие отступа

Расстояние от точки до гиперплоскости можно посчитать при помощи следующего выражения:

$\text{расстояние} = \frac{|\text{скалярное произведение}|}{\text{некоторое положительное число}}$

Чем больше скалярное произведение, тем больше расстояние от точки до гиперплоскости. Это интерпретируется следующим образом: чем больше скалярное произведение, тем более уверенный ответ классификатора.

![](images/LEC_16_PART_02/000129s_top_7.jpg)

**Отступ** — это произведение целевой переменной на значение скалярного произведения. Отступ будет положительным только в случае, если классификатор даёт верный ответ.

## Функция потерь

Попробуем построить функцию потерь, основанную на отступе. Наивным образом это можно сделать так:

![](images/LEC_16_PART_02/000189s_top_5.jpg)

Запишем функцию потерь как сумму индикаторов того, что модель ошибается, а именно, что знак скалярного произведения не соответствует значению целевой переменной. Если мы хотим переписать функцию потерь в виде отступа, то функция потерь будет выглядеть как сумма индикаторов того, что отступ меньше 0.

Однако у этой функции потерь есть существенный недостаток: она не дифференцируема. Решение, которое часто используется в машинном обучении, — оценить такую недифференцируемую функцию потерь другой, дифференцируемой функцией, и минимизировать так называемую верхнюю оценку на функцию потерь.

![](images/LEC_16_PART_02/000219s_top_8.jpg)

## Логистическая функция потерь

Логистическая регрессия использует логистическую функцию потерь, которая выглядит следующим образом:

![](images/LEC_16_PART_02/000229s_top_4.jpg)

Используя логистическую функцию потерь, задачу можно переписать так, как указано на слайде. Так как данная функция потерь является дифференцируемой, то такую задачу мы уже решать умеем. Можно просто запустить на ней градиентный спуск.

Для логистической функции потерь градиенты выглядят следующим образом, а значит, процедура градиентного спуска позволит сойти к нам к локальному минимуму.

![](images/LEC_16_PART_02/000289s_top_2.jpg)

## Мягкая и жёсткая классификация

В статистике логистическая регрессия предсказывала вероятности, а здесь мы строим предсказание модели как знак скалярного произведения. Скалярное произведение плохо подходит для предсказания вероятностей, просто потому что оно может выдавать число больше, чем единица или меньше, чем ноль.

Попробуем решить эту проблему следующим образом: возьмём выход модели в функцию, которая выдаёт число от 0 до 1. В данном случае в качестве такой функции можно использовать сигмоид.

Можно показать, что функция потерь, которую мы рассматривали до этого, эквивалентна следующей функции потерь, которая записывается через сигмоид, а значит, обучая модель на скалярное произведение, мы обучаем её и направленному предсказанию вероятностей.

Предсказание вероятности в машинном обучении выделяется в отдельный класс задач, который называется мягкой классификацией. При решении задачи мягкой классификации мы предсказываем вероятность принадлежности классу. При решении задачи жёсткой классификации мы предсказываем сам класс. Заметим, что мы всегда можем перейти от задачи мягкой классификации к задаче жёсткой классификации, просто установив некоторые порог на вероятность, а вот обратный переход либо невозможен, либо достаточно сложен.

```mermaid
classDiagram
    Классификация <|-- МягкаяКлассификация
    Классификация <|-- ЖёсткаяКлассификация
    class Классификация {
        +предсказание
    }
    class МягкаяКлассификация {
        +предсказание вероятности
    }
    class ЖёсткаяКлассификация {
        +предсказание класса
    }
```

Диаграмма выше иллюстрирует взаимосвязь между мягкой и жёсткой классификацией, где обе являются подклассами общей классификации.